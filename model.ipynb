{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Lstm, self).__init__()\n",
    "\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.LSTM(input_size, hidden_size, batch_first=True),\n",
    "        )\n",
    "        self.out_layer_1 = nn.Sequential(\n",
    "            nn.LSTM(hidden_size, hidden_size, dropout=0.2, batch_first=True),\n",
    "        )\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, 2, dropout=0.1, batch_first=True)\n",
    "        # self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # self.lrelu = nn.LeakyReLU()\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        y_lstm, _ = self.out_layer(x)\n",
    "        y_lstm = self.norm(y_lstm)\n",
    "        # y_1, _ = self.out_layer_1(y_lstm)\n",
    "        # y_1 = self.norm_1(y_1)\n",
    "        y = self.dense(y_lstm)\n",
    "        # out = self.out(y[:,-1,:])\n",
    "        return y[:,-1,:], 0, 0\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, attention_size, dropout=0.2):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.m = input_size\n",
    "        self.attention_size = attention_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.K = nn.Linear(in_features=self.m, out_features=self.attention_size, bias=False)\n",
    "        self.Q = nn.Linear(in_features=self.m, out_features=self.attention_size, bias=False)\n",
    "        self.V = nn.Linear(in_features=self.m, out_features=self.attention_size, bias=False)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=self.attention_size, out_features=self.m, bias=False),\n",
    "            # nn.Tanh(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = torch.t(self.K(x).squeeze(0))  # ENC (n x m) => (n x H) H= hidden size\n",
    "        Q = torch.t(self.Q(x).squeeze(0))  # ENC (n x m) => (n x H) H= hidden size\n",
    "        V = torch.t(self.V(x).squeeze(0))\n",
    "\n",
    "        logits = torch.div(torch.matmul(K.transpose(1, 0), Q), torch.tensor(np.sqrt(self.attention_size)))\n",
    "        weight = F.softmax(logits, dim=-1)\n",
    "        # weight = F.sigmoid(logits)\n",
    "        mid_step = torch.matmul(V, weight.t())\n",
    "        # mid_step = torch.matmul(V, weight)\n",
    "        attention = torch.t(mid_step).unsqueeze(0)\n",
    "\n",
    "        attention = self.output_layer(attention)\n",
    "\n",
    "        return attention, weight\n",
    "\n",
    "\n",
    "class SelfAttentionLstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, model_type):\n",
    "        super(SelfAttentionLstm, self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.LSTM(input_size, hidden_size, batch_first=True),\n",
    "        )\n",
    "        self.out_layer_1 = nn.Sequential(\n",
    "            nn.LSTM(hidden_size, hidden_size, batch_first=True),\n",
    "        )\n",
    "        self.out_layer_2 = nn.Sequential(\n",
    "            nn.LSTM(hidden_size, hidden_size, batch_first=True),\n",
    "        )\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, 2, dropout=0.1, batch_first=True)\n",
    "        # self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "        # self.sensor_att = SensorLstm(input_size)\n",
    "        # self.lrelu = nn.LeakyReLU()\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.att_layer = SelfAttention(input_size=hidden_size, attention_size=hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x, beta = self.sensor_att(x)\n",
    "        y_lstm, _ = self.out_layer(x)\n",
    "        y_lstm = self.norm(y_lstm)\n",
    "        if self.model_type < 9:\n",
    "            y_lstm, _ = self.out_layer_1(y_lstm)\n",
    "            y_lstm = self.norm_1(y_lstm)\n",
    "        # y_lstm, _ = self.out_layer_2(y_lstm)\n",
    "        # y_lstm = self.norm_2(y_lstm)\n",
    "        attention1, weight = self.att_layer(y_lstm)\n",
    "        # out1 = self.norm_2(attention1 + y_1)\n",
    "        y = self.dense(attention1)\n",
    "        # out = self.out(y[:,-1,:])\n",
    "        return y[:,-1,:], weight[-1], weight[:,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
